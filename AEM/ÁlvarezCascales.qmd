# CARACTERÍSTICAS CORPORALES DE HOMBRES ADULTOS Y SU RELACIÓN CON LA GRASA ACUMULADA.

### INTRODUCCIÓN

El dataset en el que se basa este informe, recoge medidas de la circunferencia de varias partes del cuerpo de 252 hombres, junto con su altura, su peso, su densidad (en el agua) y su porcentaje de grasa corporal.

Este conjunto de datos se recopiló con la pretensión de poder sacar conclusiones acerca de como afecta el aumento de grasa acumulada, en la estructura corporal de los hombres.

Es de resaltar que una variedad de libros sobre salud sugieren que los lectores evalúen su salud, al menos en parte, estimando su porcentaje de grasa corporal. En Bailey (1994), por ejemplo, el lector puede estimar la grasa corporal a partir de tablas utilizando su edad y varias mediciones de pliegues cutáneos obtenidas mediante el uso de un calibrador. Otros textos proporcionan ecuaciones predictivas para la grasa corporal utilizando medidas de circunferencia corporal (por ejemplo, circunferencia abdominal) y/o mediciones de pliegues cutáneos. Véase, por ejemplo, Behnke y Wilmore (1974), pp. 66-67; Wilmore (1976), p. 247; o Katch y McArdle (1977), pp. 120-132).

Para finalizar esta introducción, resulta relevante mencionar que porcentajes de grasa corporal altos, llevan a la persona a padecer sobrepeso y obesidad, condiciones que acarrean riesgos severos para la salud como presión arterial alta (hipertensión), colesterol LDL alto, colesterol HDL bajo o niveles altos de triglicéridos (dislipidemia), diabetes tipo 2, enfermedad coronaria, derrame cerebral o apnea del sueño y problemas respiratorios entre otros, que dan lugar a una baja calidad de vida y reducción del bienestar.

Se considera que un hombre de **entre 20 y 39 años** tendrá sobrepeso si su porcentaje de grasa corporal está entre el 20 y 25% y será obeso si el porcentaje supera el 25%. Pero si su edad está **entre 40 y 59 años,** el sobrepeso se da con un 22-28% y la obesidad con cifras superiores al 28%. Y para los mas mayores, por encima de los **65 años,** el sobrepeso está entre el 25 y 30%, mientras que obesidad en porcentajes superiores al 30% de grasa corporal.

Por otro lado, la cuantía mínima de grasa corporal para la vida humana es de entorno al 3%. Es el porcentaje mínimo necesario para lograr realizar funciones vitales del organismo.

Además, es destacable mencionar que normalmente las disciplinas deportivas tienen unos estándares de porcentaje de grasa corporal asociados. Por ejemplo, en el caso del fútbol, **la media se establece en el 10%**. Un delantero como Ronaldo se sitúa por debajo de este porcentaje, lo que favorece su agilidad. No obstante, un defensa puede tener una cantidad de grasa corporal algo superior para soportar mejor los choques. Destacan sobretodo las pruebas de **resistencia extrema**, como maratones, por ser donde se encuentran los deportistas con menor porcentaje de grasa corporal, con números que se acercan al 4%. También en el culturismo o en el ciclismo. Alberto Contador, por ejemplo, en su momento de estado óptimo rondaba esta cifra.

### DATASET

El conjunto de datos se llama \`bodyfat.csv' y presenta 15 variables que recogen datos de 252 hombres.

Las variables recogidas son:

```{r}
d <- read.csv('bodyfat.csv')
colnames(d)
```

i).Density: Variable cuantitativa que se refiere a la [densidad del cuerpo]{.underline} determinada mediante un pesaje [bajo el agua]{.underline}. Se mide en [g/cm\^3]{.underline}

ii)BodyFat: [Porcentaje de grasa corporal]{.underline}.

iii).Age: [Edad]{.underline} del individuo, en [años.]{.underline}

iv).Weight: [Peso del individuo]{.underline}, medido en libras. (lb)

v).Height: [Altura]{.underline} en pulgadas (in)

vi).Neck: [Circunferencia del Cuello.]{.underline} (cm)

vii). Chest: [Circunferencia del Pecho]{.underline}. (cm)

viii).Abdomen: [Circunferencia del Abdomen]{.underline}. (cm)

ix).Hip: [Circunferencia de la cadera]{.underline}. (cm)

x).Thigh: [Circunferencia del Muslo]{.underline}. (cm)

xi).Knee: [Circunferencia de la Rodilla.]{.underline} (cm)

xii). Anckle: [Circunferencia del Tobillo]{.underline}. (cm)

xiii).Biceps: [Circunferencia del Biceps.]{.underline} (cm)

xiv).Forearm: [Circunferencia del Antebrazo]{.underline}. (cm)

xv).Wrist: [Circunferencia de la Muñeca.]{.underline} (cm)

El conjunto de datos proviene de la página web kaggle (<https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset>)

El dataset fue inicialmente distribuido por el Doctor A. Garth Fisher, quien dió permiso para redistribuirlo con fines no comerciales. Concretamente la persona que lo subió a la web lo obtuvo del Departamento de Matemáticas y Ciencias de la Computación de la Escuala de minas y Tecnología de Sur Dakota.

*Roger W. Johnson*

*Department of Mathematics & Computer Science*

*South Dakota School of Mines & Technology*

*501 East St. Joseph Street*

*Rapid City, SD 57701*

### ESTUDIO INICAL DE LOS DATOS

-   **Primeramente conviene visualizar un resumen de las variables** (dado que son 15 variables y puede que no se visualicen bien, lo hacemos en 3 grupos)

```{r, echo = FALSE}
summary(d[,1:5])
summary(d[,6:10])
summary(d[,11:15])
```

Resulta peculiar ver que en el resumen de la variable BodyFat, (porcentaje de grasa corporal), haya individuos con un porcentaje de grasa igual a 0, cuando en la busqueda por internet vimos que la cuantía mínima de grasa corporal para la vida humana es del 3%.

```{r}
sum(d$BodyFat < 3)
```

```{r, warning=FALSE, echo = FALSE}
individuosAtípicos <- subset(d, d$BodyFat <3)
individuosAtípicos
```

Concretamente hay [dos individuos para los que el porcentaje de grasa corporal, cae por debajo del mínimo necesario para la vida.]{.underline} Estas observaciones atípicas son las de los individuos 172 y 182, con porcentajes de grasa corporal del 0.7 y del 0 % respectivamente. Leyendo en la propia página Kaggle, de la que proviene el conjunto de datos, resulta que muy probablemente haya una medición errónea en la densidad corporal de estos dos individuos. Por ahora solo los tendremos en cuenta, pero no los eliminamos ya que nos pueden servir en pasos posteriores.

Entre otros aspectos a resaltar del resumen de las variables, cabe mencionar que la edad de los participantes en este estudio va desde los 22 años a los 89. Esto, como se menciona en la introducción, es algo relevante a tener en cuenta, ya que conforme la edad aumenta es común aumentar también el porcentaje de grasa.

-   **Visualizando las variables en gráficos de cajas y bigotes:**

```{r, fig.width=5 , fig.height=4, echo = FALSE}
par(mfrow=c(1, 3))
#Primera gráfica
boxplot(d[,1:5])
text(c(1,2,3,4,5),d[182,1:5],labels = "182", col = 'red')
text(c(1,2,3,4,5), d[172, 1:5],labels = "172", col = 'blue')
#Segunda Gráfica
boxplot(d[,6:10])
text(c(1,2,3,4,5), d[182, 6:10],labels = "182", col = 'red')
text(c(1,2,3,4,5), d[172, 6:10],labels = "172", col = 'blue')
#Tercera Gráfica
boxplot(d[,11:15])
text(c(1,2,3,4,5), d[182, 11:15],labels = "182", col = 'red')
text(c(1,2,3,4,5), d[172, 11:15],labels = "172", col = 'blue')
```

Nótese que los individuos con valores anormales del porcentaje de grasa corporal (182 y 172) no destacan realmente en el resto de variables, aunque si que es cierto que presentan valores bajos, en casi todas ellas. Esto nos indica que son individuos con un tamaño corporal pequeño, es decir no son corpulentos.

Se aprecian también valores atípicos, la gran mayoría por exceso, esto no significa que tengamos datos erróneos, sino que nos informa de la presencia de individuos con partes del cuerpo bastante grandes. En principio los atípicos al no ser muchos, no deben de suponer ningún problema para realizar las diferentes técnicas de análisis que emplearemos.

-   **Escalas de las variables**

    Aunque en el apartado anterior ya se apreciaba como algunas variables podrían no pertenecer a la misma escala, conviene visualizar el gráfico de cajas y bigotes conjuntamente para todas las variables.

    ```{r, echo = FALSE, fig.width=5, fig.height=4}
    boxplot(d)
    ```

    A simple vista para las variables más hacia la derecha, se aprecia como sí podrían estan dentro de la misma escala. Pero mirando el total del conjunto, se diferencia bastante que tienen escalas distintas.

    Por ejemplo, como se muestra en el resumen del apartado anterior, la densidad corporal toma valores en un rango entre 0.995 y 1.109 g/cm3, mientras que de media la circunferencia del pecho es de 100.82 cm, o la de la muñeca es 18.23 cm

    Además, si calculamos la desviación estandar de las variables, vemos como las observaciones, se desvían de forma bastante distinta respecto a sus medias, obviando que tienen escalas distintas.

    ```{r}
    desviaciones_estandar <- apply(d, 2, sd)
    desviaciones_estandar
    ```

    Por tanto es conveniente estandarizarlas antes de trabajar con ellas.

    La estandarización de variables es una práctica bastante común en el análisis de datos que puede mejorar la interpretación, la estabilidad numérica y la comparabilidad de los modelos estadísticos y de aprendizaje automático.

-   **Relación entre variables:**

    Resulta conveniente poder conocer si las variables se encuentran bien relacionadas entre sí, o si por el contrario son más independientes.

```{r,fig.width=7 , fig.height=7, echo = FALSE}
plot(d)
```

A primera vista ya vemos como la densidad y la grasa corporal son variables inversamente relacionadas con una correlación notablemente fuerte. También se aprecia cierta relación lineal entre algunas partes del cuerpo, lo que es entendible al ser partes del cuerpo de una misma persona. Sería extraño que un individuo presentase algunas partes del cuerpo anormalmente grandes frente a otras muy pequeñas.

Veamos mas de cerca para algunas variables en concreto:

**a).Variables de las medidas corporales:**

[a1. Abdomen y Cintura]{.underline}

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(d$Abdomen, d$Hip, ylab = 'Hip', xlab = 'Abdomen')
text(d$Abdomen,d$Hip ,cex=0.6, labels = row.names(d))
```

```{r}
cor(d$Hip, d$Abdomen)
```

Se aprecia una fuerte correlación entre la circunferencia del abdomen y la cintura, esto queda explicado claramente al ser partes del cuerpo muy cercanas. Tendrían cuerpos muy descompensados y anormales si presentasen tamaños muy distintos de la cadera y el abdomen.

[a2. Antebrazo y Muñeca]{.underline}

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(d$Forearm, d$Wrist, )
```

En cuanto al antebrazo y la muñeca, la relación sigue siendo notablemente visible, aunque ya no se aprecia tan directa como en el caso del abdomen y la cintura por ejemplo.

[a3. Tobillo y Cuello]{.underline}

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(d$Ankle, d$Neck, ylab = 'Neck', xlab = 'Ankle')
```

```{r}
cor(d$Ankle, d$Neck)
```

En el caso del cuello y el tobillo al ser partes del cuerpo bastante distanciadas, la relación no es tan fuerte.

[En general las variables que miden el tamaño de partes del cuerpo suelen estar bastante bien relacionadas. Lo cual era de esperar, ya que las partes del cuerpo de una misma persona,]{.underline} salvo por anomalías o casos extremos[, tienden a presentan una cierta armonía. Y más aún si son partes del cuerpo pertenecientes a zonas próximas.]{.underline}

**b).Peso y Altura:**

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(d$Weight, d$Height, ylab = 'Altura', xlab = 'Peso')
text(d$Weight,d$Height ,cex=0.6, labels = row.names(d))
```

```{r}
cor(d$Weight, d$Height)
```

La altura y el peso parecen no guardar una relación tan significativa como la que presentan entre sí las variables que miden el tamaño de miembros del cuerpo. [Podemos apreciar como un aumento en el peso no se traduce necesariamente en un aumento de altura.]{.underline}

[Destaca sobretodo el individuo 216]{.underline} por tener un mayor peso que la gran mayoría de hombres del estudio, pero siendo una persona de poca estatura.

```{r, echo = FALSE}
d[216, 4:5]
```

Mas concretamente este individuo (216) es, el más bajo y pesa más que el 75% de los demás hombres del estudio.

**c).Grasa Corporal y Tamaño del Abdomen**

```{r, echo = FALSE, fig.width=5, fig.height=4}
#Creo un modelo de regresión lineal simple para poder visualizar la línea
modelo <- lm(d$Abdomen ~ d$BodyFat)
plot(d$BodyFat, d$Abdomen)
text(d$BodyFat,d$Abdomen ,cex=0.6, labels = row.names(d))
abline(modelo, col = "green")
```

```{r}
cor(d$BodyFat, d$Abdomen)
```

Esta fuerte correlación entre el aumento de la grasa corporal y el aumento de la circunferencia del abdomen, muy probablemente se deba a que es precisamente en esa parte del cuerpo donde tiende a acumularse la grasa corporal en los hombres.

Si visualizamos la relación entre la grasa corporal y cualquier otra parte del cuerpo, por ejemplo los muslos, vemos como la relación es efectivamente más debil que con el abdomen. Indicando que se agrupa mas tejido graso en el abdomen que en los muslos.

(La línea verde representa la relación entre la grasa corporal y el tamaño abdomen, mientras que la roja la relación entre la grasa corporal y el tamaño del muslo, para así evidenciar que la primera es más fuerte que al segunda)

```{r, echo =FALSE, fig.width=5, fig.height=4}
modelo2 <- lm(d$Thigh ~ d$BodyFat)
plot(d$BodyFat,d$Thigh, xlab = 'BodyFat', ylab = 'Thigh')
text(d$BodyFat,d$Thigh, cex=0.6, labels = row.names(d))
text(d$BodyFat[50],d$Thigh[50], cex = 0.5, label = '50', col = 'blue')
text(d$BodyFat[39],d$Thigh[39], cex = 0.5, label = '39', col = 'red')
abline(modelo2, col = "red")
abline(modelo, col = "green")
```

En ambas gráficas destaca el individuo 39 (en rojo), por tener un alto porcentaje de de grasa corporal, pero sobretodo por tener un tamaño corporal bastante por encima del resto de participantes del estudio. Y como por el contrario, obviando a los individuos 182 y 172 que eran los que tenían una medición de la grasa corporal errónea, el 50 (en azul) es uno de los que menos grasa tiene y menor tamaño corporal.

**d).Grasa Corporal y Peso:**

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(d$BodyFat, d$Weight)
```

El porcentaje de grasa corporal y el peso, si que guardan relación, aunque no es tan fuerte como en otras variables. Esto puede que se explique ya que, el tejido adiposo que compone la grasa corporal no es tan pesado (0,9 kg/L) como el tejido muscular (1,06 kg/L). Por lo que un [aumento de grasa corporal, sí que supondrá un aumento de peso del individuo, pero no tan significativo como un aumento de tejido muscular por ejemplo.]{.underline}

(fuente densidad de tejidos : [https://es.wikipedia.org/wiki/Tejido_adiposo#:\\](https://es.wikipedia.org/wiki/Tejido_adiposo#:)%7B.uri%7D\~:text=El%20tejido%20adiposo%20tiene%20una,%2C06%20kg%2FL).

**e). Densidad y tamaño del abdomen**

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(d$Abdomen, d$Density)

```

La firme relación inversa entre la desidad corporal y el tamaño del abdomen puede tener su explicación en unas características anteriormente mencionadas. En un apartado anterior se pode de manifiesto que la fuerte relación entre el tamaño del abdomen y el porcentaje de grasa corporal, se debe a que principalmente la grasa en hombres se acumule en esa zona del cuerpo. Teniendo en cuenta que el tejido adiposo, que conforma la grasa corporal, es menos pesado que el tejido muscular y óseo. Esto da lugar a que un aumento de la grasa corporal, influya proporcionalmente más en un aumento del tamaño del abdomen, que en el peso del individuo.

Siendo la densidad, una magnitud referida a la cantidad de masa en un determinado volumen (m/V), aquellos hombres con mayor porcentaje de grasa corporal presentarán un mayor volumen en proporción a su masa total, que los que preseten un menos grasa corporal.

Lo que se traduce en que, un mayor tamaño del abdomen implica más espacio ocupado en proporción al peso, y por lo tanto menor densidad.

**f).Densidad Corporal y Edad**

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(d$Density, d$Age)
```

También hay variables que son bastante idependientes entre sí como es el caso de la edad con la desidad del cuerpo bajo el agua.

En concluisón, vemos como las variables de las partes del cuerpo, están bastante correladas, lo que puede ser una ventaja o un inconveniente. Debemos tener en cuenta que si se construye un modelo predictivo, hay que tener cuidado al usar variables altamente correlacionadas, ya que pueden llevar a problemas de multicolinealidad.

-   **Distribución que siguen las variables**

Podemos visualizar las variables, con qqnorm() para intentar ver si presentan una distribución normal. Que los datos presenten o no una distribución normal, puede resultar significativo en apartados siguientes de este informe. Si los datos provienen de una distribución normal, los puntos en el gráfico Q-Q estarán aproximadamente a lo largo de la línea recta.

```{r, echo = FALSE, fig.show='hold', fig.width=5, fig.height=3, warning=FALSE}
library(tidyverse)
#par(mfrow=c(1, 2))
par(mfrow=c(1, 3))
# Calcula la densidad y traza el gráfico para cada variable en d
for (variable in names(d[,7:15])) {
  
qqnorm(d[[variable]], main = paste("Gráfico de", variable), col = "skyblue")
qqline(d[[variable]])

}
```

### ANÁLISIS DE LAS COMPONENTES PRINCIPALES

El Análisis de Componentes Principales (PCA) se emplea para resumir la información contenida en muchas variables aleatorias (relacionadas) y las principales características de sus individuos, en unas pocas variables denominadas componentes principales.

Durante el estudio inicial ya vimos como había algunas variables que no solo empleaban unidades distintas, sino que sus valores se movían en rangos diferentes al resto, es decir que presentaban escalas distintas. A la hora de aplicar un PCA, para evitar que tengan más importancia las variables con escalas mayores, es conveniente que los datos se estandaricen, usando la matriz de correlaciones. De esta forma todas tienen a priori la misma importancia (varianza uno).

Además, para evitar que los individuos con valores de su porcentaje de grasa corporal inferior al mínimo vital (172 y 182), afecten a las interpretaciones de las componentes, es conveniente proseguir el estudio sin ellos.

```{r}
data <- d[-c(172,182), ]
#Para aplicar la matriz de correlaciones el parámetro cor debe tomar valor TRUE
PCA <-princomp(data, cor = TRUE)
summary(PCA)
```

-   **Selección de las Componentes Principales**

La importancia de las componentes se mide con las proporciones en tanto por uno de sus varianzas (proportion of Variance) y las proporciones acumuladas.

Para seleccionar cuantas componentes debemos tomar, podemos atender a distintos criterios. Uno de los más comunes es determinar un [porcentaje mínimo]{.underline} de variabilidad de la información inicial, que deban abarcar las componentes, y quedarnos con número de componentes suficiente que permita alcanzar ese porcentaje mínimo. Para este caso, un porcentaje de entre el 75 y 80% parece una opción bastante apropiada. Esto sería tomar 3 componentes principales.

Otro critero que se conoce como uno de los más comunes en estadística es [la Regla del codo.]{.underline} Esta regla establece que serán representativas las componentes hasta el primer "codo" (sin incluirlo) de la gráfica o hasta que comience la línea recta aproximada final.

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(eigen(cor(data))$values,type='l',ylab='valores propios', col = 'blue')
```

Al visualizar la gráfica vemos claramente un cambio de la tendencia de la pendiente en el punto 2, y otro no tan drástico en torno al punto 3. Lo cual nos está indicando que deberíamos de quedarnos con 1 o 2 componentes.

Un último criterio que empleamos para desempatar es la [regla de Kaiser,]{.underline} la cual nos dice que solo serán relevantes las componentes que tengan una variabilidad mayor que la variabilidad media de las variables originales. Al haber usado la matriz de correlaciones, como es equivalente a usar las variables estandarizadas, se entiende que las varianzas iniciales son 1. Entonces solo debemos coger aquellas componentes con una desviación por encima de 1, que en este caso son [las 3 primeras.]{.underline}

-   **Interpretación de las componentes principales:**

Para dar una interpretación de las componentes debemos mirar las cargas o loadings, que son los vectores propios normalizados. Estos actúan como los coeficientes de las variables iniciales en la expresión que nos permite calcular la componente principal. Por lo tanto como el valor de la componente depende en gran medida de estos coeficientes, los debemos analizar para poder darles un significado.

#### i).Primera componente

```{r, echo=FALSE}
#Los loadings de la primera componente los podemos visualizar con el siguiente código:
L <- PCA$loadings
L[,1]
```

La expresión de la primera componente principal quedaría como:

Y1 = 0.223 · X1\* -0.234· X2\* -0.019· X3\* -0.326· X4\* -0.128· X5\* -0.284· X6\*- 0.305 · X7\*- 0.307 · X8\* - 0.31 · X9\* -0.292 · X10\* - 0.289 · X11\* - 0.208 · X12\* - 0.279 · X13\* - 0.228 · X14\*- 0.253 · X15\*

En esta componente la variable Edad (Age) no se encuentra bien representada, dado que tiene un valor muy próximo a 0. También nos encontramos con que la variable Altura (Height) tiene una menor relevancia dentro de la componente que el resto de variables (valor absoluto de 0.128, frente a más de 0.200 del resto)

Destaca que las variables de circunferencia corporal (como la del abdomen, cadera, pecho, etc.) tengan una importancia similar dentro de la componente, estando mas o menos igualmente representadas y además están asociadas (negativamente) con la grasa corporal.

La densidad corporal es la única variable que presenta un valor positivo, es decir, esta componente primera es directamente proporcional al aumento de la densidad, mientras que inversamente proporcional al crecimiento del resto de variables como son el aumento de las partes del cuerpo, y el porcentaje de grasa.

Esto sugiere que esta componente es **principalmente un indicativo de las medidas corporales y la densidad y a su vez de como influye la distribución de la grasa corporal en el tamaño de partes del cuerpo.**

Entonces presentará [valores pequeños cuanto más corpulento sea el indivividuo, y valores grandes cuanto menor tamaño corporal y menos porcentaje de grasa tenga (porque será más denso).]{.underline}

#### ii).Segunda Componente

```{r, echo =FALSE}
L[,2]
```

En esta segunda componente principal no están bien representadas ni el Peso (Weight), ni el tamaño del Cuello (Neck), Cadera (Hip) y Biceps. Además, el resto de variables relacionadas con la circunferencia de partes del cuerpo, no presentan una influencia excesivamente significativa.

Mientras que, las variables con los loadings más altos y por tanto fuertemente relacionadas con la componente son, la Densidad corporal, la Altura, el porcentaje de Grasa corporal y la Edad. Teniendo la Densidad y la Altura coeficientes positivos y la Grasa y la Edad, coeficientes negativos, además de encontrarse prácticamente igual representadas.

Cabe destacar que la Densidad y Grasa Corporal, están estrechamente relacionadas entre sí, como se expone en apartados anteriores, dado que un aumento de la grasa corporal supone un cambio más significativo en el volumen del cuerpo, que en el peso total de este, propiciando disminución de la densidad. O viéndolo desde otro enfoque, una alta densidad indica una mayor proporción de masa magra en comparación con la grasa corporal. Esto concuerda con que estén contrarrestándose entre sí en la componente (distinto signo).

Por otro lado, la edad y la altura son factores que influyen en la composición y estructura corporal. La edad podría representar el efecto del envejecimiento en el cuerpo, como señalábamos en la introducción. Y la altura (comúmente usada en otros campos como un indicador de la estructura corporal), podría estar relacionada con la distribución de la masa corporal, de forma que una altura mayor, pueda significar una distribución diferente de la grasa y la masa magra, dando lugar a tamaños diferentes de las distintas partes del cuerpo.

Por lo tanto, esta segunda componente principal representa **la variabilidad en la composición corporal** (más tejido muscular o graso), **y la influencia de la edad y la altura en esta composición.**

Un [valor alto de la componente indicará alta densidad, gran altura y un porcentaje de grasa corporal bajo, mientras que valores bajos, lo contrario, es decir, poca densidad y alto porcentaje de grasa corporal.]{.underline}

#### iii).Tercera Componente

```{r, echo = FALSE}
L[,3]
```

En esta componente las circunferencias de las partes del cuerpo no son muy significativas, estando la mayoría poco representadas al tener coeficientes cercanos a 0.

También vuelve a darse en esta componente, aunque de una forma menos destacada, la relación inversa entre la densidad corporal y el porcentaje de grasa, que explicabamos para la componente anterior.

Y sobretodo tiene una altísima relevancia la variable Edad. Por lo que esta componente **es principalmente un indicativo de la edad del varón**. De la forma que [tomará valores mas altos, para aquellos individuos más longevos.]{.underline}

-   **Puntuaciones de los Individuos para las componentes principales**

Estas puntuaciones nos permiten tener una imagen de como son los individuos, segun los valores que toman en esa componente

```{r}
S <- PCA$scores
```

Podemos analizar, algunos de los individuos con puntuaciones más extremas en las componentes.

Por ejemplo para la componente primera, el individuo con la menor puntuación es el 39.

```{r}
which.min(S[,1])
```

Recordemos que tener una puntuación baja en esta componente estaba relacionado con tener un gran tamaño de las partes del cuerpo, y además una gran porcentaje de acumulación de grasa.

```{r, echo = FALSE}
d[39,]
```

Efectivamente, viendo los valores de este individuo 39 en las variables inicales, denota un tamaño corporal por encima de la media. Por ejemplo, la cintura de media tiene 99.9 cm de circunferencia y la de este hombre son 147.7 cm, o su peso es de 363 libras que equivale a unos 160 kg.

Por el contrario, mirando al individuo con mayor puntuación en la primera componente, el 45. Se diferencia del 39, al presentar un tamaño corporal pequeño, y bajo porcentaje de grasa, llevando a su vez a una densidad mayor.

```{r}
which.max(S[,1])
```

```{r, echo = FALSE}
d[45,]
```

También podemos hacer lo propio para la componente segunda:

El individuo con la máxima puntuación en la componente seguda es:

```{r}
which.max(S[,2])
```

```{r, echo = FALSE}
d[12,]
```

Tener una puntuación alta en esta segunda componente era sinónimo de una densidad alta, gran altura y bajo porcentaje de grasa corporal. Vemos como efectivamente este individuo 12 con la máxima puntuación, tiene una altura de 76 pulgadas que son 193 cm, un bajo porcentaje de grasa corporal, y además, presenta alta densidad (el 75% de los hombres del estudio tiene una densidad inferior a 1.07 g/cm3) .

Finalmente, podemos analizar los siguientes gráficos que muestran las puntuaciones de los individuos para las dos primeras componentes principales.

```{r, echo = FALSE}
par(mfrow=c(1, 2))
plot(S[,1],S[,2], xlab= "Comp. Y1", ylab = "Comp. Y2")
text(S[,1],S[,2], labels = row.names(d),cex=0.6)
text(S[45,1],S[45,2], cex = 0.6, label = '45', col = 'blue')
text(S[12,1],S[12,2], cex = 0.6, label = '12', col = 'red')

biplot(PCA, pc.biplot = TRUE, xlabs = 1:250)
```

En el primer gráfico (izquierda), los individuos con tamaño corporal pequeño y poca acumulación de grasa se encuentran más hacia la derecha (en la componente primera), como es el caso del 45, y aquellos con mayor altura y densidad se encuentran más arriba (en la componnete segunda), como es el caso del individuo 12.

El segundo gráfico (derecha) es similar al primero, pero las variables aparecen como vectores en rojo y las puntuaciones estandarizadas como las etiquetas de los datos en negro. En este, las variables presentan vectores largos, lo que es sinónimo de que se encuentran bien representadas por las dos primeras componentes. Si tuvieran vectores cortos , es indicativo de que la información se pierde al ser proyectada, por ser casi perpendiculares, lo que sería sintoma de estar mal representadas.

Con este segundo gráfico se comprueba la interpretación hecha al gráfico anterior, al ver al individuo 45, que presentaba un tamaño corporal pequeño, en dirección contraria a los vectores de las variables que tasan el tamaño de partes del cuerpo. O al individuo 39, que se caracteriza por tener un tamaño corporal atípicamente grande, estando muy hacia la izquierda, donde confluyen la mayoría de vectores de las variables que miden el tamaño de partes del cuerpo.

Una vez tenemos las puntuaciones de todos los individuos y para todas las componentes, debemos tomar únicamente las puntuaciones de aquellas con las que elegimos quedarnos, (1,2y 3). De esta forma podríamos emplearlas, según convenga, para futuras técnicas que llevemos a cabo, en vez de utilizar las 15 variables de nuestro dataset original.

```{r}
Puntuaciones3Comp <- data.frame(Comp1 = S[,1], Comp2 = S[,2], Comp3 = S[,3])
```

-   **Saturaciones**

Para poder conocer la información total que se tiene de cada variable en las componentes debemos calcular las saturaciones al cuadrado. Estas nos indicarán cuanta información (en tanto por 1) habrá en cada componente de cada variable.

```{r}
SAT<-cor(data,S)
```

Además para conocer el porcentaje total de cada variable entre las 3 componentes con las que nos hemos quedado, debemos calcular las comunalidades. En nuestro caso, se calculan como la suma de las saturaciones al cuadrado de las 3 componentes principales.

```{r, echo=FALSE}
COM <- SAT[,1]^2 + SAT[,2]^2 + SAT[,3]^2
COMUNALIDADES <- data.frame(SAT[,1]^2,SAT[,2]^2,SAT[,3]^2,COM)
colnames(COMUNALIDADES) <- c("Inf. Comp1", "Inf. Comp2", "Inf Comp3", "Comunalidad")
```

```{r}
COMUNALIDADES
```

Entre las 3 componentes, el porcentaje de información que recogen en general de cada variable es bastante alto, superando el 80%, en la mayoría de casos. Auque las variables Ankle y Forearm, son de las que peor porcentaje de información se conserva, superando levemente el 50 %.

### ANÁLISIS CLÚSTER: APRENDIZAJE NO SUPERVISADO

En el gráfico que se visualizó de las puntuciones de los individuos respesto a las dos primeras componentes principales, vimos como una mayoría tendía a agruparse hacia la derecha y en torno al centro del gráfico, mientras que había otros individuos que se localizaban en posiciones más alejadas de esta agrupación principal, lo cual sugiere, que muy probablemente, podamos distinguir a los individuos de este estudio entre grupos.

```{r, echo = FALSE}
plot(S[,1],S[,2], xlab= "Comp. Y1", ylab = "Comp. Y2")
text(S[,1],S[,2], labels = row.names(d),cex=0.6, col = "red")
```

Intetaremos agrupar a los individuos del dataset según la similitud de sus valores en las variables. Al no existir grupos iniciales, este tipo de técnica se denominan análisis no supervisado o aprendizaje automático. La formación de los grupos dependerá de las distancias usadas para medir las similitudes y del algoritmo de agrupación aplicado.

Existen principalmente dos algoritmos para llevar a cabo la clasificación de los individuos, K-means (análisis cluster con k-means) y h-clust (análisis cluster jerarquizado). Para el primer algoritmo es necesario indicar inicialmente el número de grupos o clusters entre los que agrupar. Mientras que para el segundo no es necesario.

En nuestro caso, a partir del estudio anterior realizado, **no es sencillo determinar a simple vista un número concreto de grupos en el que estarían repartidos los individuos del dataset,** por lo que aplicaremos el **análisis jerarquizado por h-clust**. Con este algortimo, un dendrograma nos indicará los grupos que se van formando y nos permitirá decidir con cuántos nos quedamos.

El funcionamiento de este algoritmo se basa en calcular las distancias de cada uno de los individuos a todos los demás, e ir agrupando iteración por iteración a las observaciones que más cercanas se encuentren entre sí. Cada individuo forma un cluster inicialmente, y con que se agrupen dos observaciones, ya tendremos un nuevo cluster, que se irá ampliando con las demás observaciones o clusters similares que tengan una distancia cercana.

La idea es poder distinguir entre varias clases que se diferencien según el tipo de cuerpo del individuo y su composición/estructura.

Este análisis cluster podría llevarse a cabo solo con los datos de las puntuciones de los individuos en las 3 primeras componentes pricipales que recopilamos anteriormente, sin embargo dado que el número de variables iniciales no es muy alto (15), lo haremos también con las variables originales, permitiéndonos clasificar a los individuos antendiendo al 100% de la información total. Y pudiendo comparar resultados.

#### **1.Aplicación del análisis jerarquizado (hclust) al dataset original**

```{r}
ds <- scale(data) #Estandarizar los datos: Ya se introdujo en apartados anteriores la importancia de estandarizar los datos.
```

```{r}
Distancias <- dist(ds, method = 'euclidean')
#Aplicamos la distancia euclídea
```

```{r}
CA <- hclust(Distancias, method = "complete")
```

```{r, echo = FALSE}
plot(CA,cex=0.8,main='Dendograma',ylab='Distancia',xlab='Observaciones',sub='')
abline(h=10,col='red')
abline(h=9,col='green')
```

El dendograma nos muestra como se han ido formando las distintas agrupaciones, y nos permite decidir a simple vista en cuantas clases posibles podríamos dividir a los individuos del dataset.

Debido al gran número de individuos resulta dificil diferenciar a simple vista los dos individuos que primero se agruparon, pero sacándolo por medio de la matriz de distancias, vemos que se trata de los individuos 225 y 228.

Sí que está muy claro que el último en ser agrupado fue el 39, lo cual indica claramente su tamaño atípicamente grande respecto al resto de participantes.

```{r}
M <- as.matrix(Distancias)[1:250,1:250]
min(Distancias)
# Encontrar la posición del valor en la matriz
posicion <- which(M == min(Distancias), arr.ind = TRUE)
# Imprimir la fila y la columna donde se encuentra el valor
print(paste("Fila:", posicion[,1], "Columna:", posicion[,2]))
```

**1.1Número de grupos a escoger**

Mirando el dendrograma, parece una buena opción tener entre 4 y 5 grupos. Realmente no existe un número óptimo, sino que es algo que depende en gran medida de factores subjetivos, pero existen técnicas que pueden ayudarnos a decidir.

Hay índices implementados en R que nos pueden ayudar a decidirnos por un número de grupos en concreto.

```{r, results='hide', fig.show='hold', fig.width=4, fig.height=4}
library(NbClust)
NbClust(ds,method='complete',index='all')$Best.nc
#Escondo el resultado para evitar ocupar innecesariamente espacio
```

En nuestro caso, se indica que hay una mayoría de 9 índices que determinan que el mejor el número de clusters es 5, aunque también hay un segundo conjunto de 8 índices que determinan el óptimo en 2 clusters.

Agrupamos por tanto en 5 clusters.

```{r}
#Agrupando en 5 grupos
g5 <- cutree(CA, k = 5)
```

```{r, echo = FALSE}
#5Grupos
plot(S[,1],S[,2], xlab= "Comp. Y1", ylab = "Comp. Y2")
text(S[,1],S[,2], labels = row.names(data),cex=0.6, col = g5)
```

Vemos como el atípico 39, da lugar a la creación de un grupo para poder clasificarlo únicamente a él. Esto a la hora de llevar a cabo algunas técnicas como análisis discriminante, puede suponer un grave desbalanceo que afecte gravemente a las conclusiones que se obtengan. Por lo tanto debemos evaluar realmente la relevancia del atípico en el análisis, ¿Es un error de medición o un punto realmente significativo?

Realmente este indivividuo 39 no es un error de medición, sino que simplemente es alguien que presenta un tamaño corporal bastante superior a lo común. Esto es algo perfectamente posible en la realidad.

Sin embargo, este grupo de un único individuo quizás esté distorsionando los resultados, por lo que para hacer un correcto análisis cluster resulta conveniente excluirlo del dataset a la hora de clasificar. Por ello vamos a repetir el proceso sin el atípico 39.

```{r, echo = FALSE}
Distancias <- dist(ds[-c(39),], method = 'euclidean')
CA <- hclust(Distancias, method = "complete")
plot(CA,cex=0.8,main='Dendograma',ylab='Distancia',xlab='Observaciones',sub='')
abline(h=10,col='red')
abline(h=9,col='green')
```

Al eliminar el atípico 39, ya no se refleja en el dendrograma. A simple vista sigue pareciendo buena idea la clasificación entre 4 y 5 grupos.

```{r, echo=FALSE,fig.width=4, fig.height=4}
NbClust(ds[-c(39),],method='complete',index='all')$Best.nc
```

Sin el atípico, un mayoría de 8 índices indican 4 como el mejor número de clusters. Lo que concuerda con el resto de grupos que no estaban formados por un atípico.

Agrupando en 4 clusters:

```{r}
g4 <- cutree(CA, k = 4)
```

```{r, echo = FALSE}
#4Grupos
plot(S[-c(39),1],S[-c(39),2], xlab= "Comp. Y1", ylab = "Comp. Y2")
text(S[-c(39),1],S[-c(39),2], labels = row.names(data[-c(39),]),cex=0.6, col = g4)
```

El algoritmo hclust ha hecho la misma clasificación que antes de eliminar el atípico, lo cual es lógico ya que las distancias entre los distintos individuos sigue siendo la misma a pesar de considerar un individuo menos.

Debemos recopilar la agupación hecha como un único dataset para su posterior uso:

```{r}
#Creamos dos dataset nuevos para guardar la clasificación de los datos sin el atípico 39, para evitar así eliminarlo del todo por si nos puede servir en algún paso siguiente.
dataClasified <- data.frame(data[-c(39),],g4)
dsClasified <- data.frame(ds[-c(39),],g4)
```

**1.2 Descripción de los grupos.**

Si llevamos a cabo un breve estudio de las características de los grupos nos va a permitir describirlos.

```{r, echo = FALSE, results='hide'}
# Lista para almacenar los resúmenes de cada variable
resumenes <- list()

# Bucle para iterar sobre cada variable en 'data'
for (variable in names(dataClasified[,-c(16)])) {
  # Aplicar la función tapply para obtener el resumen de la variable respecto a 'grupo'
  resumen_variable <- tapply(dataClasified[[variable]], dataClasified$g4  , summary)
  # Almacenar el resumen en la lista
  resumenes[[variable]] <- resumen_variable
}

# Mostrar los resúmenes de cada variable
print(resumenes)

```

```{r, echo=FALSE, fig.show='hold'}
par(mfrow=c(2, 3))
for (variable in names(dataClasified[,-c(16)])) {
  #plot(data[[variable]],data$g5,pch=20, xlab = paste(variable), ylab = "grupos")
  boxplot(dataClasified[[variable]]~dataClasified$g4, pch = 20,ylab = paste(variable), xlab = "grupos" )
  
}
```

```{r, echo=FALSE, fig.show='hold'}
par(mfrow=c(1, 2))
#Represento algunas de las gráficas de las circunferencias de partes del cuerpo más significativas
plot(dataClasified$Chest, dataClasified$Abdomen, pch = as.integer(dataClasified$g4))
legend('bottomright', legend = c('1','2','3','4'), pch = 1:4)

plot(dataClasified$Hip, dataClasified$Abdomen, pch = as.integer(dataClasified$g4))
legend('bottomright', legend = c('1','2','3','4'), pch = 1:4)

plot(dataClasified$Neck, dataClasified$Chest, pch = as.integer(dataClasified$g4))
legend('bottomright', legend = c('1','2','3','4'), pch = 1:4)

plot(dataClasified$Thigh, dataClasified$Biceps, pch = as.integer(dataClasified$g4))
legend('bottomright', legend = c('1','2','3','4'), pch = 1:4)
```

Atendiendo al resumen de las variables respecto a cada grupo y a las anteriores gráficas podemos describir los grupos en los que se clasifican a los individuos de la siguiente forma:

```{r, echo = FALSE, fig.width=5, fig.height=4}
plot(S[-c(39),1],S[-c(39),2], xlab= "Comp. Y1", ylab = "Comp. Y2")
text(S[-c(39),1],S[-c(39),2], labels = row.names(dataClasified),cex=0.6, col = g4)
```

[Grupo1]{.smallcaps}. (Negro): De media es el grupo con más densidad, y que menor porcentaje de grasa corporal presenta. Además sus individuos tienen el menor peso corporal. Y en cuanto al tamaño, son los individuos con menor circunferencia de las partes del cuerpo. En conclusión, son los **hombres menos corpulentos y con el menor porcentaje de grasa corporal.**

[Grupo2. (Rojo):]{.smallcaps}Los individuos de este grupo presentan un porcentaje de grasa corporal repartido, con valores tanto altos como bajos, pero con una mayoría de individuos con un porcentaje moderado o no muy alto , entre el 17 y el 20 %, y un peso corporal medio/bajo. El tamaño de las distintas partes del cuerpo es en general también intermedio o medio/bajo, pero por encima de los individuos del primer grupo. Se puede resumir en **individuos con un tamaño corporal medianamente pequeño (medio/bajo), y con un porcentaje de grasa mayoritariamente moderado.**

[Grupo3]{.smallcaps}. (Verde) En este grupo los individuos presentan un porcentaje de grasa corporal bastante repartido al igual que en el grupo 2, pero mayoritariamente algo mayor, en un rango medio/alto , en torno al 22 - 24%. Presentan un peso corporal más bien alto, junto con un tamaño corporal medio/alto. En conclusión, son **hombres con un tamaño corporal medianamente grande, y porcentaje de grasa corporal mediano. Destacan también por ser mayoritariamente el grupo de hombres más altos.**

[Grupo4]{.smallcaps}. (Azul oscuro): Grupo formado por un conjunto reducido de individuos que presentan porcentajes altos de grasa, junto con un alto peso corporal. Siendo a su vez el grupo con la densidad corporal más baja en general. Destacan por tener un tamaño corporal bastante mayor que el resto de individuos. Podría resumirse como el grupo de **los** **hombres más corpulentos, y de mayor porcentaje de grasa corporal, además de un peso corporal pesado.**

```{r, echo = FALSE}
print("Individuos del grupo 1:")
sum(dataClasified$g4 == 1)
print("Individuos del grupo 2")
sum(dataClasified$g4 == 2)
print("Individuos del grupo 3")
sum(dataClasified$g4 == 3)
print("Individuos del grupo 4")
sum(dataClasified$g4 == 4)

```

Se da claramente un desbalanceo al haber un número total de individuos tan dispar en cada grupo. Es importante tener esto en cuenta ya que al realizar algunos análisis estadísticos puede conducir a resultados sesgados o poco confiables.

#### **2.Aplicación del análisis jerarquizado (hclust) a partir de las puntuaciones de los individuos en las 3 componentes principales**

Durante el Análisis de las componentes principales, seleccionamos las 3 primeras que abarcaban un porcentaje total de la información del 80%, esto quiere decir que muy probablemente la clasificación que obtengamos en este segundo análsis cluster varíe, siendo menos precisa.

Obtenemos el siguiente dendrograma:

```{r, echo = FALSE}
Distancias2 <- dist(Puntuaciones3Comp[-c(39),], method = 'euclidean')
CA2 <- hclust(Distancias2, method = "complete")
plot(CA2,cex=0.8,main='Dendograma',ylab='Distancia',xlab='Observaciones',sub='')
abline(h=12,col='red')
abline(h=9,col='green')
```

Parece que lo mejor sería tomar entre 2 y 4 grupos.

**2.1Número de grupos a escoger**

```{r, echo = FALSE, fig.width=5, fig.height=4}
library(NbClust)
NbClust(Puntuaciones3Comp,method='complete',index='all')$Best.nc
```

En este caso la mayoría de índices indican que el mejor número de clusters es 2.

```{r}
#Agrupando en 2 grupos
g2 <- cutree(CA2, k = 2)
```

```{r, echo = FALSE, fig.show='hold'}
par(mfrow=c(1, 2))
#2Grupos
plot(S[-c(39),1],S[-c(39),2], xlab= "Comp. Y1", ylab = "Comp. Y2")
text(S[-c(39),1],S[-c(39),2], labels = row.names(dataClasified),cex=0.6, col = g2)

plot(S[-c(39),1],S[-c(39),2],pch=as.integer(g2),col=g2,cex=0.7)
```

Finalmente añadimos la clasificación hecha a los dataset.

```{r}
#Le quito el 39 porque así lo había hecho en los dos dataset anteriores
Puntuaciones3Comp <- data.frame(Puntuaciones3Comp[-c(39),],g2)
dataClasified <- data.frame(dataClasified, g2)
dsClasified <-data.frame(dsClasified,g2)
```

**2.2 Descripción de los grupos.**

Al existir solamente dos grupos, el estudio de las características de estos se simplifica bastante.

```{r, echo=FALSE}
#La gráfica anterior se puden entender muy bien con esta al lado
biplot(PCA, pc.biplot = TRUE, xlabs = 1:250)
```

```{r, echo =FALSE,  results='hide'}
# Lista para almacenar los resúmenes de cada variable
resumenes <- list()

# Bucle para iterar sobre cada variable en 'data'
for (variable in names(dataClasified[,-c(16,17)])) {
  # Aplicar la función tapply para obtener el resumen de la variable respecto a 'grupo'
  resumen_variable <- tapply(dataClasified[[variable]], dataClasified$g2, summary)
  # Almacenar el resumen en la lista
  resumenes[[variable]] <- resumen_variable
}

# Mostrar los resúmenes de cada variable
print(resumenes)
```

```{r, echo=FALSE, fig.show='hold'}
par(mfrow=c(2, 3))
for (variable in names(dataClasified[,-c(16,17)])) {
  #plot(data[[variable]],data$g2,pch=20, xlab = paste(variable), ylab = "grupos")
  boxplot(dataClasified[[variable]]~dataClasified$g2, pch = 20,ylab = paste(variable), xlab = "grupos" )

  
}
```

Con tan solo dos grupos, la clasificación queda algo pobre, al no ser tan precisa como la anterior hecha con 4 grupos.

En este caso tenemos un primer grupo (en color negro en la gráfica) en el que se clasifican los hombres con un porcentaje de de grasa tanto pequeño como intermedio, peso corporal entre pequeño y mediano, mayor densidad y con un tamaño corporal tanto pequeño como intermedio, es decir no excesivamente corpulentos. Y un segundo grupo que aglutina a aquellos con un mayor peso corporal, porcentaje de grasa alto, menor densidad y los más corpulentos.

[Grupo1.]{.smallcaps} Los **hombres con un tamaño corporal tanto pequeño como mediano y un porcentaje graso entre bajo y medio.**

[Grupo2]{.smallcaps}. Los **hombres que destacan por tener una mayor corpulencia y porcentaje de grasa que el resto.**

```{r, echo = FALSE}
print("Individuos del grupo 1:")
sum(dataClasified$g2 == 1)
print("Individuos del grupo 2")
sum(dataClasified$g2 == 2)
```

#### **3.¿Cómo hubiera clasificado el algoritmo kmeans para el mismo número de grupos?**

Finalemente podemos comparar la clasificación en 4 grupos hecha por hclust con la que haría Kmeans

```{r, echo = FALSE, fig.show='hold'}
set.seed(1234)
CA1<-kmeans(ds[-c(39),],centers=4,nstar=10)
#Clasificación por Kmeans
library(cluster)
clusplot(ds[-c(39),],CA1$cluster,color=T,shade=T,labels=2,cex=0.5,lines=0, main = NULL)
title(main = 'Clasificación por kmeans')

#Clasificación por hclust
plot(S[-c(39),1],S[-c(39),2], xlab= "Comp. Y1", ylab = "Comp. Y2")
text(S[-c(39),1],S[-c(39),2], labels = row.names(dataClasified),cex=0.6, col = g4)
title(main = 'Clasificación por hclust')
```

Añadimos la clasificación a los dataset

```{r}
Y <- CA1$cluster
dataClasified <- data.frame(dataClasified, Y)
dsClasified <- data.frame(dsClasified,Y)
```

**3.2 Descripción de los grupos.**

Visualizamos las variables respecto a los grupos formados por kmeans, lo que nos permite describir las caracaterísticas de estos.

```{r, echo = FALSE, results='hide'}
# Lista para almacenar los resúmenes de cada variable
resumenes <- list()

# Bucle para iterar sobre cada variable en 'data'
for (variable in names(dataClasified[,-c(16,17,18)])) {
  # Aplicar la función tapply para obtener el resumen de la variable respecto a 'grupo'
  resumen_variable <- tapply(dataClasified[[variable]], dataClasified$Y, summary)
  # Almacenar el resumen en la lista
  resumenes[[variable]] <- resumen_variable
}

# Mostrar los resúmenes de cada variable
print(resumenes)
```

```{r, echo=FALSE, fig.show='hold'}
par(mfrow=c(2, 3))
for (variable in names(dataClasified[,-c(16,17,18)])) {
  #plot(data[[variable]],data$g2,pch=20, xlab = paste(variable), ylab = "grupos")
  boxplot(dataClasified[[variable]]~dataClasified$Y, pch = 20,ylab = paste(variable), xlab = "grupos" )
}

```

La clasficación por el algortimo kmeans es distinta a la hecha anteriormente con hclust. Esto se debe a que kmeans, como primer paso determina los centroides iniciales de los k grupos al azar, agrupando a los individuos según su cercanía a estos centroides (de ahí la semilla). Lo que da lugar a que aunque los centroides se vuelvan a calcular tras cada iteración, haya una componente de aleatoriedad en este algoritmo y quizás para un conjunto de datos pequeño como este, en el que los grupos no están tan bien definidos, no sea tan preciso. Además de que puede dar lugar a una peor clasificación, dado que se vea forzado a incluir algún atípico en un grupo, teniendo que desplazar o modificar los 'límites' de otros grupos.

[-Una descripción de los grupos hecha por kmeans sería:]{.underline}

```{r, echo = FALSE, fig.width=5, fig.height=4}
clusplot(ds[-c(39),],CA1$cluster,color=T,shade=T,labels=2,cex=0.5,lines=0, main = NULL)
title(main = 'Clasificación por kmeans')
```

[Grupo1]{.smallcaps}. (Verde): Es un grupo en el que sus individuos presentan un **porcentaje de grasa corporal intermedio, junto con un tamaño corporal mediano y estatura entre media y baja.**

[Grupo2. (Naranja):]{.smallcaps} Se trata del grupo que engloba a los **hombres de mayor tamaño corporal, así como un porcentaje de grasa mayoritariamente más alto** que el resto de grupos, también presentan **gran estatura.**

[Grupo3]{.smallcaps}. (Azul): En este grupo los individuos presentan un **porcentaje de grasa corporal medio/bajo**, con un **tamaño corporal mediano**, pese a ser los **individuos mayoritariamente más altos.** Además tienen un **peso corporal medio bajo.**

[Grupo4]{.smallcaps}. (Rosa): Este grupo lo componen los **hombres de menor tamaño y peso corporal**, que además presentan el **menor porcentaje de grasa** corporal.

Para esta nueva clasificación, tenemos [dos grupos que engloban a los individuos de mayor y menor tamaño corporal,]{.underline} estos son el 2 y el 4, cuyas características se asemejan a las de los grupos 4 y 1 del hclust respectivamente. Y además, [se tiene dos cluster para aquel subconjunto de hombres con una corpulencia intermedia, pero que se diferencian mayoritariamente por su edad y altura.]{.underline}

Siendo estos últimos los cluster 1 y 3. Son bastante similares, presentando ambos un tamaño corporal mediano/pequeño, pero el primero de ellos destacando por componerse de indviduos más longevos y de menor estatura, mientras que el otro por tener individuos más jovenes, más altos y con un porcentaje de grasa corporal algo más bajo.

```{r, echo = FALSE}
print("Individuos del grupo 1:")
sum(dataClasified$Y == 1)
print("Individuos del grupo 2")
sum(dataClasified$Y == 2)
print("Individuos del grupo 3")
sum(dataClasified$Y == 3)
print("Individuos del grupo 4")
sum(dataClasified$Y == 4)
```

Con kmeans sigue habiendo un problema de desbalanceo, pero este se reduce bastante.

------------------------------------------------------------------------

En coclusión, con **hclust los grupos son más representativos, los individuos quedan mejor delimitados según sus atributos corporales** **que con kmeans** permitiéndonos definir/describir mejor las características de estos. **Pero con kmeans se da un menor balanceo lo que pueda repercutir en resultados más fiables a la hora de clasificar nuevos individuos.**

------------------------------------------------------------------------

### ANÁLISIS DISCRIMINANTE

Con tal de [validar si los grupos obtenidos anteriormente por análisis cluster son realmente distinguibles, así como, cual de las dos algoritmos obtiene resultados más precisos (hclust o kmeans) y además descubrir qué variable es la más influyente a la hora de clasificarlos]{.underline}, vamos a llevar a cabo un análisis discriminante.

Aplicamos el análisis discriminante con las variables estandarizadas, ya que de esta forma tendrán valores similares, y así los coeficientes obtenidos con ellas en el análisis se podrán usar para estudiar la influencia de las variables en la clasificación.

-   **¿LDA o QDA?**

Primeramente debemos intentar decidir qué método es el óptimo para nuestros datos. En función de las características de estos debemos determinar cual de los dos métodos, LDA o QDA, podría retornar mejores resultados.

Será buena opción aplicar LDA si las matrices de covarianzas de cada grupo son iguales o muy similares, mientras que la opción del QDA será más acertada si las variables usadas para clasificar son normales (multivariantes) en cada grupo.

```{r}
library('MASS')
library('mvnormtest')
#Comprobando para el LDA
grupo1 <- dsClasified[dsClasified$g4 == 1, 1:15]
s1 <- cov(grupo1)
grupo2 <- dsClasified[dsClasified$g4 == 2, 1:15]
s2 <- cov(grupo2)
grupo3 <- dsClasified[dsClasified$g4 == 3, 1:15]
s3 <- cov(grupo3)
grupo4 <- dsClasified[dsClasified$g4 == 4, 1:15]
s4 <- cov(grupo4)

```

```{r, echo = FALSE, results = FALSE}
print(s1)
print(s2)
print(s3)
print(s4)
```

\*\*Véase, en el archivo de código aportado, como las matrices de covarianzas no coinciden, son bastante diferentes. No se adjunta la salida de las matrices al ocupar mucho espacio.

```{r}
#Comprobación del QDA.
mshapiro.test(t(grupo1))
mshapiro.test(t(grupo2))
mshapiro.test(t(grupo3))
mshapiro.test(t(grupo4))
```

Tampoco pasan el test de normalidad para poder confirmar la aplicación del QDA.

Por ello, viendo que ninguna de las dos comprobaciones, ni la del LDA, ni la del QDA, se cumple, podemos optar por emplear ambas y quedarnos con la que mejor clasifique los datos.

**`1.LDA`**

La función para aplicar el LDA presenta un parámetro 'prior', que permite definir las probabilidades de perternecia a priori a cada clase. Se suele emplear cuando se conocen dichas probabilidades y son fáciles de expresar. En este caso, al estar tan desbalanceados los grupos es preferible que no se fije ninguna probabilidad a priori, de forma que se estime automáticamente utilizando la proporción de observaciones en cada clase en los datos de entrenamiento.

Además, [el análisis se lleva a cabo por validación cruzada]{.underline}, de forma que no tengamos que diferenciar entre dos conjuntos de test o entrenamiento, sino que es la propia función la que hace particiones de todo el conjunto de datos y va tomando por cada iteración todas las particiones menos una, para dedicarlas a entrenamiento y la que excluyó la emplea para testear. Esta es una práctica común cuando la muestra no es de gran tamaño.

-   **Análisis discriminante lineal para la clasificación hecha por hclust**

```{r}
LDACV<-lda(dsClasified[,1:15],dsClasified$g4,CV=TRUE)
matrizConfusion1<-table(dsClasified$g4,LDACV$class,dnn=c("real","predicho"))
matrizConfusion1
```

```{r}
accuracy1 <- sum(diag(matrizConfusion1)) / sum(matrizConfusion1)
accuracy1
```

-   **Análisis discriminante lineal para la clasificación hecha por Kmeans**

```{r}
LDACV2<-lda(dsClasified[,1:15],dsClasified$Y,CV=TRUE)
matrizConfusion2<-table(dsClasified$Y,LDACV2$class, dnn=c("real","predicho"))
matrizConfusion2
```

```{r}
accuracy2 <- sum(diag(matrizConfusion2)) / sum(matrizConfusion2)
accuracy2
```

**`2.QDA.`**

Al igual que con LDA no vamos a definir las probabilidades de perternecia a priori para cada clase. También se lleva acabo por validación cruzada.

-   **Análisis discriminante cuadrático para la clasificación hecha por hclust.**

```{r}
#QDACV<-qda(dsClasified[,1:15],dsClasified$g4,CV=TRUE)
#table(dsClasified$g4,QDACV$class) 
```

No deja aplicar el QDA para la clasificación hecha por hclust, dado que hay un grupo demasiado pequeño, como es el grupo 4 de hclust, compuesto por 15 individuos.

-   **Análisis discriminante cuadrático para la clasificación hecha por Kmeans.**

```{r}
QDACV2<-qda(dsClasified[,1:15],dsClasified$Y,CV=TRUE)
matrizConfusion3 <- table(dsClasified$Y,QDACV2$class, dnn=c("real","predicho")) 
matrizConfusion3
```

```{r}
accuracy3 <- sum(diag(matrizConfusion3)) / sum(matrizConfusion3)
accuracy3
```

**`3.Recopilación Resultados`**

Finalmente, comparamos los resultados obtenidos:

Para el LDA, la eficiencia es:

-   $211/249 = 0.85$ si cojo la clasificación de **hclust**
-   $226/249 = 0.9$ si cojo la clasificación de **kmeans**

Para el QDA, la eficiencia es:

-   No me permite coger los datos del hclust por haber un grupo con pocos individuos, lo cual es un problema que deriva directamente del desbalanceo que sufren los datos.
-   $189/249 = 0.75$ si cojo la clasificación de **kmeans.**

Para los datos con los que trabajamos, resulta mejor la aplicación de un análisis discriminante lineal, ya que se obtienen mejores resultados con este que con el cuadrático. Además de que el análisis cuadrático no permite trabajar con grupos pequeños, de ahí el error que se muestra al intentar aplicarlo para la clasificación hecha por el hclust, que tiene un grupo con solo 15 individuos.

Fijándonos solamente el el análisis discriminante lineal, aunque la diferencia no es muy significativa, la clasificación de los datos agrupados por el algortimo Kmeans resulta ser algo más eficiente que para la agrupación por hclust. Obteniédose una exactitud del 85% en la clasificación de los grupos de hclust, frente a un 90% en la clasificación de los grupos de kmeans. Esto puede que se deba a que la agrupación hecha por Kmeans presenta un menor debalanceo que a la de hclust.

-   **Conocer las variables más influyentes a la hora de discriminar**

Para la agrupación que ha resultado ser la más exacta a la hora de discriminar, merece poder conocerse cuales son las variables que más influyen a la hora de distinguir si un individuo debe pertenecer a uno u otro grupo.

```{r, echo=FALSE}
LDACVVar<-lda(dsClasified[,1:15],dsClasified$Y)
```

```{r, echo=FALSE}
LDACVVar$scaling
```

Al mostrar la matriz de 'scaling' tenemos 3 componentes discriminantes. Estos son combinaciones lineales de variables predictoras que utiliza el Análisis Discriminante para discriminar entre los diferentes grupos en un conjunto de datos.

Cada componente discriminante captura una cierta cantidad de información sobre la estructura de separación entre las clases en los datos. Y se ordenan según su capacidad para discriminar entre las clases. El primer componente discriminante captura la mayor parte de la variabilidad entre las clases 87.71%, el segundo un 10.41% y el tercero 1.88%.

Entonces, atendiendo a que el primer componente discriminante es el de mayor capacidad de discriminación, las variables más relevantes son la [densidad corporal, y el tamaño del pecho y muslo]{.underline}.

### REGRESIÓN LINEAL MÚLTIPLE:

Dado que la principal motivación con la que se recopilaron los datos de este estudio, fue para sacar conclusiones acerca de la estructura corporal de los hombres, vamos a llevar a cabo un análisis de regresión lineal múltiple. De esta forma podremos obtener un modelo que nos permita encontrar una forma de predecir el tamaño de alguna parte del cuerpo atendiendo al resto de medidas.

La regresión lineal la llevaremos a cabo con el dataset data, que no contempla las columnas de calsificación añadidas en apartados anteriores. Tampoco podemos aplicar los datos estandarizados ya que algunas funciones que usaremos (como boxcox()) no aceptan trabajar con una variable respuesta que presente valores negativos, como ocurre al estandarizar los datos.

#### [**1.Predicción de la variable Biceps**]{.underline}

En el estudio incial, con la gráfica qqnorm() vimos como la variable Biceps era una de las que mejor parecía seguir una variable normal, por lo que intentaremos encontrar un modelo que prediga el tamaño del biceps del hombre en función del resto de variables.

**1.1 Análisis previo de la variable respuesta**

Analizaremos brevemente la normalidad de la variable Biceps, y veremos si requiere de alguna transformación en los datos.

```{r, echo = FALSE, fig.width=5, fig.height=4}
boxplot(data$Biceps)
text(c(1),data$Biceps[39],labels = "39", col = 'red')
```

Vemos de nuevo al individuo 39 como un atípico para el tamaño del biceps, lo cual, nos pueda suponer un problema a la hora de que los datos pasen el test de normalidad

$$
H_0: BodyFat \text{ sigue una distribución normal}
$$

$$
H_1: BodyFat \text{ NO sigue una distribución normal}
$$

```{r, echo = FALSE}
library('MASS')
library('mvnormtest')
mshapiro.test(t(data$Abdomen))
```

Probamos sin el atípico 39, a ver si así pasa en test de normalidad

```{r, echo = FALSE}
mshapiro.test(t(data$Abdomen[-c(39)]))
```

Debemos rechazar que Biceps sigua una distribución normal. Ni excluyendo al atípico parece seguir una normal. Muy probablemente esto se deba a que la variable requiera de una transformación en sus datos (por la familia Box-Cox). Para identificar la transformación más adecuada sobre la variable respuesta, usaremos la función boxcox() del paquete MASS. Indicar que la familia de transformaciones de Box-Cox consiste en elevar la variable respuesta a un exponente λ \> 0, o bien tomar logaritmos neperianos si resulta λ = 0.

```{r, echo = FALSE, fig.width=5, fig.height=4}
library("MASS")
boxcox(lm(Biceps ~ 1, data = data), lambda = seq(-3, 3, 1/10))
```

Se observa que la transformación más adecuada es la logarítmica. Por lo que creamos otra variable nueva, que sea la transformación de Biceps, a la que le volveremos a comprobar la normalidad. Y si pasa el test, será nuestra nueva variable respuesta.

```{r}
data$BicepsNew <- log(data$Biceps) #Transfomación
mshapiro.test(t(data$BicepsNew))   #Comprobación de la normalidad
```

Aceptamos la normalidad de la variable resultante de la transformación, ' BodyFat' , por lo que el modelo se contruye para predecir esa variable.

Antes de finalizar este primer apartado de análisis de la variable respuesta, cabe recordar que ya vimos en el estudio inicial de los datos, como algunas variables presentaban una fuerte relación lineal, lo cual nos podría resultar en problemas de multicolinealidad.

**1.2 Construcción de un modelo inicial.**

Primeramente, vamos a definir 2 subconjuntos, uno de entrenamiento o train, con el que construiremos el modelo y otro de test, con el que evaluaremos que tal predice nuestro modelo. De esta forma podremos evaluar qué tal se comporta con datos nuevos, teniendo así una métrica más fiable de qué tan bueno es el modelo.

```{r}
set.seed(1234)
indices_entrenamiento <- sample(1:nrow(data), 0.8*nrow(data))
entrenamiento <- data[indices_entrenamiento,]
test <- data[-indices_entrenamiento,]
```

Construimos un modelo inicial con todas las variables, que nos pueda servir como base a partir de la cual aplicamos métodos de selección de regresores con tal de conseguir el mejor modelo posible.

```{r, warning=FALSE}
library("rms")
library("Hmisc")
modelo_inicial <- lm(BicepsNew ~ Density + BodyFat + Age + Weight + Height + Neck + Chest + Abdomen + Hip + Thigh + Knee + Ankle + Forearm + Wrist, data = entrenamiento)
summary(modelo_inicial)
```

```{r}
vif(modelo_inicial)
```

Vemos como el modelo presenta una bondad de ajuste relativamente buena, por encima del 0.73.

Podemos también comprobar la multicolinealidad al calcular el factor de varianza inflada, para los predictores. Este factor (VIF) mide la relación lineal de cada predictor con el resto de predictores. Valores de este factor que resulten por encima de 7 son indicativo de la existencia de multicolinealidad. Entonces, sabemos que hay variables presentando multicolinealidad.

Destacar también que hay bastantes regresores con los p-valores correspondientes a si los coeficientes son significativos, tomando un valor alto, superior a 0.10. Esto, junto con que haya variables que presentan multicolinealidad, es indicativo de que el modelo es claramente reducible.

**1.3 Aplicación de los métodos de selección de regresores**.

Aplicaremos los métodos de selección de regresores backward, fordward y stepwise, para intenatr optener un modelo más reducido.

**a). Modelo Backward:**

```{r, results= 'hide'}
modelo_backward<- step(modelo_inicial, direction = "backward")
```

**b).Modelo Forward**

Para aplicar los modelos forward y stepwise, tendremos que partir de un modelo que contempla sólo la constante.

```{r}
modelo_cte <- lm(BicepsNew ~ 1 , data = data)
```

```{r, results = 'hide'}
modelo_forward <- step(modelo_cte, direction = "forward", scope = formula(modelo_inicial))

```

**c). Modelo Stepwise**

```{r, results = 'hide'}
modelo_stepwise <- step(modelo_cte, direction = "both",scope = formula(modelo_inicial))
```

-   **Comparación de los modelos**

    Se observa que el modelo obtenido por backward es distinto al obtenido por los otros dos métodos.

```{r}
modelo_backward$coefficients
modelo_forward$coefficients 
modelo_stepwise$coefficients
```

Debemos comparar la bondad de ajuste de los dos métodos obtenidos para así quedarnos con el mejor.

```{r}
summary(modelo_backward)
summary(modelo_forward)
```

Optaremos por quedarnos con el modelo backward ya que es el que mejor bondad de ajuste presenta.

```{r, echo = FALSE, results='hide'}
modelo_final <- lm(formula = BicepsNew ~ Weight + Height + Neck + Hip + Thigh + 
    Forearm, data = entrenamiento)
```

En este modelo final, no se mejora la bondad de ajuste respecto al modelo inicial. Presenta un valor de R-cuadrado ajustado de 0.734, lo que indica que este modelo explica en un 73% la variabilidad del tamaño del biceps de un hombre. Además, no todos los regresores de este modelo final son significativos, al tener uno de ellos, Neck, un p-valor algo superior a 0.10. Esto nos indica que [el modelo sigue siendo reducible.]{.underline}

**1.4 Validación del modelo**

Antes de poder emplear el modelo libremente para predecir, debemos cerciorarnos de que es lo suficientemente bueno, como para considerar fiables y válidas las predicciones que se lleven a cabo con él. Se debe verificar que los residuos sigan una distribución normal y que sean independientes, que la varianza es constante en las perturbaciones aleatorias (Hipótesis de Homocedasticidad) y que no se de multicolinealidad entre variables predictoras ni existan observaciones que influyan mucho más que el resto.

***a).Observaciones influyentes***

Para comprobar que no hay observaciones influyentes, podemos calcular la distancia de Cook para cada observación. Como regla empírica, valores por encima de 1 indican que se trata de una observación influyente. En general, conviene representar los valores de la distancia de Cook con el fin de identificar si hay alguna observación con valores especialmente altos comparados con el resto.

```{r, fig.width=5, fig.height=4}
cook <- cooks.distance(modelo_final)
plot(cook)
text(cook,cex=0.6, labels = row.names(entrenamiento))
```

***b).Multicolinealidad***

```{r, echo = FALSE}
vif(modelo_final)
```

Vemos como Weight presenta gran multicolinealidad, debemos quitarla y ver que ocurre.

```{r}
modelo_final <- lm(formula = BicepsNew ~ Height + Neck + Hip + Thigh + Forearm, data = entrenamiento)
summary(modelo_final)
vif(modelo_final)
```

Al quitar Weight ya no hay multicolinealidad en el modelo. Además aprovechamos para reducir el número de variables predictoras al tener Height y Hip un p-valor bastante por encima de 0.1.

```{r}
modelo_final <- lm(formula = BicepsNew ~ Neck + Thigh + Forearm, data = entrenamiento)
summary(modelo_final)
```

El valor de R-cuadrado se acaba estableciendo superior al 72%.

***c).Hipótesis de Homocedasticidad***

Vemos como tiene un comportamiento aleatorio con dispersión aproximadamente constante

```{r, echo =FALSE, fig.width=5, fig.height=4}
plot(modelo_final$fitted.values, modelo_final$residuals)
text(modelo_final$fitted.values, modelo_final$residuals,cex=0.6, labels = row.names(entrenamiento))
```

***d)Hipótesis de Independencia***

Analizamos la independencia con el test de Durbin-Watson y representando la serie temporal de residuos.

```{r, echo = FALSE, warning = FALSE}
library("lmtest")
dwtest(modelo_final, alternative="two.sided")
```

Al tener un p-valor superior a 0.10 en el test de Durbin-Watson, podemos suponer independencia de las observaciones.

```{r, warning = FALSE, fig.width=5, fig.height=4, echo = FALSE}
ts.plot(modelo_final$residuals)
```

***e).Test de Normalidad de los residuos***

```{r, echo = FALSE}
shapiro.test(modelo_final$residuals)
```

El modelo pasa el test de normalidad de los residuos.

Se nos queda finalmente el siguiente modelo:

```{r, echo = FALSE}
modelo_final$coefficients
```

```{r,echo=FALSE}
summary(modelo_final)
```

**1.5 Intento de mejora del modelo**

Una vez que hemos comprobado que tenemos un modelo válido que pasa todas las comprobaciones necesarias podemos intentar probar a ajustarlo más, elevando a alguna potencia las variables predictoras.

No debemos pasarnos, por lo que entraríamos en problemas de sobreajuste.

```{r}
modelo_final_mejorado <- lm(formula = BicepsNew ~ I(Neck^4) + Thigh + pol(Forearm,3), data = entrenamiento)
summary(modelo_final_mejorado)
```

Al aplicar un polinomio de grado 3 a la variable Forearm, y elevar a potencia 4 a la variable Neck, conseguimos mejorar un poco el ajuste hasta superar el 74%.

Siendo finalmente los coeficientes del modelo mejorado:

```{r, echo=FALSE}
modelo_final_mejorado$coefficients
```

**1.6 Predicciones**

Ahora debemos probar a emplear el modelo para ver qué tal trabaja.

Podemos emplearlo para dar un intervalo de los valores posibles que pueden tomar los coeficientes. Cualquier modelo teórico con coeficientes incluidos en el intervalo siguiente serían también válidos teniendo en cuenta nuestros datos muestrales

```{r, echo=FALSE}
#Intervalos de confianza al 95% para los parámetros del modelo_final_mejorado
confint(modelo_final_mejorado, level=0.95)
```

Para la predicción empleando nuevos valores, utilizaremos la función predict que permitirá obtener tanto valores de predicción como intervalos de confianza para el valor medio de la respuesta y para el valor de la respuesta utilizando la opción en el argumento interval = "confidence" e interval = "prediction", respectivamente.

Una de las prácticas más comunes en el ámbito de la ciencia de datos es probar con los datos de test, los modelos construidos con los datos de entrnamiento. Y tasar la eficiencia de estos calculando algunas de sus métricas.

En nuestro caso calcularemos las métricas **MSE** (Error cuadrático medio) y **MAE** (Error absoluto medio)

$$MSE = E((h(X) − Y )^2).$$

$$
MAE = E(|h(X) - Y|)
$$

```{r}
predicciones <- predict(modelo_final_mejorado, newdata = test, interval = "prediction", level = 0.95)
predicciones <-as.data.frame(predicciones)
```

Debemos de tener en cuenta que las predicciones son sobre la variable BicepsNew, y no sobre la variable original Biceps. Es decir las predicciones obtenidas están bajo la transformación de la familia Box-Cox, por lo que antes de calcular el MSE o MAE, se debe de deshacer dicha transformación.

```{r}
#Deshacer la transformación
p <- exp(predicciones$fit)
MSE <- mean((test$Biceps - p)^2)
RMSE <- sqrt(MSE)
MAE <- mean(abs(test$Biceps - p))

```

```{r}
MSE
```

```{r}
RMSE
```

```{r}
MAE
 
```

El MSE al estar elevado al cuadrado, es bastante sensible a errores de predicción, y más aún si trabaja con algún atípico. Un MSE de 3.93 unidades con respecto a las observaciones reales pueda parecer un error alto para el contexto en el que nos movemos, ya que la circunferncia del biceps está más o menos en torno a un rango de 25 a 35 cm.

Pero si le hacemos la raiz cuadrada, es decir el RMSE, vemos como esta variación entre las predicciones y observaciones reales se reduce a la mitad.

Por otro lado, el MAE, indica que, en promedio, las predicciones del modelo se desvían aproximadamente en 1.37 unidades de las observaciones reales en valor absoluto.

Teniendo esto en cuenta, podemos decir que los errores no son excesivamente grandes a pesar de que la bondad de ajuste del modelo no supere el 75%.

Podemos considerarlo como un modelo relativamente bueno, que tiene la capacidad adecuada para dar una predicción cercana a la realidad.

```{r}
summary(data$Biceps)
```

`-Para el individuo 182`

También podemos testear el modelo más a fondo y ver que nos retorna con los individuos que descartamos al inicio de este estudio. Por ejemplo emplearemos al individuo 182 con tal de predecir su tamaño del biceps en función del tamaño de su antebrazos, muslo, y cuello .

```{r}
d182 <- data.frame(Forearm = d$Forearm[182], Thigh = d$Thigh[182], Neck = d$Neck[182])
```

```{r}
#Intervalo para el valor de la respuesta
predict(modelo_final_mejorado, newdata = d182, interval = "prediction", level = 0.99)
```

```{r}
exp(3.288416)
```

```{r}
d$Biceps[182]
```

Vemos como la predicción es relativamente buena para no tener el modelo una bondad de ajuste especialmente alta, se equivoca en menos de 1 cm.

#### **2.Aplicación del Algoritmo del gradiente descendente.**

Resulta interesante poder comparar los coeficientes del modelo estimados por medio del algoritmo del gradiente, con los coeficientes obtenidos por medio de la función lm(), que emplea el método de mínimos cuadrados.

El código empleado es una modificación del ofrecido en la práctica 2 de la asignatura Análisis Estadístico Multivariante.

```{r, fig.cap='Hola'}
#Vector de unos
x0_RLM <- c(rep(1,length(entrenamiento$BicepsNew)))
#Debemos tener en cuenta que la variable respuesta del modelo es BicepsNew
```

```{r}
#Regresores
forearm <- entrenamiento$Forearm
thigh <- entrenamiento$Thigh
neck <-entrenamiento$Neck
chest <- entrenamiento$Chest
#Variable Respuesta
biceps <- entrenamiento$BicepsNew
```

```{r}
n_RLM <-length(x0_RLM)
variables<-data.frame(x0_RLM,forearm,thigh,neck,chest,biceps)
k <- 4 #Número de regresores
```

```{r}
#Metemos los datos muestrales en una matriz M (matriz de diseño)
M<-matrix(1,n_RLM,k+1)
M[,2]<-forearm
M[,3]<-thigh
M[,4]<-neck
M[,5]<-chest
#Definimos función costo usando la matriz de diseño M
h_RLM<-function(theta,x) {sum(theta*x)}
J_RLM<-function(theta,M) {0.5*sum((M %*%theta- biceps)^2)/n_RLM}
#Fijamos iteraciones, learning rate y valores iniciales
m_RLM <-50 # Número de interaciones
alfa_RLM <-0.1 # learning rate
theta_RLM <-c(1,1,1,1,1) #valores iniciales de los theta
```

```{r}
J2_RLM<- array() #Vector con actualizaciones de la función costo en cada iteración
hv_RLM<- array() #Vector con actualizaciones de los valores ajustados en cada iteración
Z2_RLM<-matrix(NA,m_RLM+1,k+1) #Matriz con actualizaciones de los theta en cada iteración
J2_RLM[1]<-J_RLM(theta_RLM,M)
Z2_RLM[1,]<-theta_RLM
```

```{r}
for (i in 1:m_RLM) {
hv_RLM<- M%*%theta_RLM
theta_RLM <- theta_RLM - (alfa_RLM/n_RLM) * t(M)%*%(hv_RLM-biceps)
J2_RLM[i+1]<-J_RLM(theta_RLM,M)
Z2_RLM[i+1,] <- theta_RLM
}
```

```{r}
#Guardamos los valores de los theta y del costo en un dataframe
resultados_RLM <- data.frame(Z2_RLM, J2_RLM)
colnames(resultados_RLM) <- c("theta0", "theta1", "theta2","theta3","theta4", "costo")

```

```{r}
A<-t(M) %*% M
B<-solve(A)
# A %*%B
thetas_exactos <-B %*% t(M) %*% biceps
```

```{r, echo =FALSE}
print('Coeficientes obtenidos con el gradiente descendente:')
print(thetas_exactos) #óptimos exactos
print('Coeficientes obtenidos con el método de mínimos cuadrados: ')
print(modelo_final$coefficients)
```

Se alcanzan por ambos métodos los mismos coeficientes del modelo. Aunque resulta más cómodo hacerlo solo por medio de la función lm() que solo requiere una única instrucción.

### Consideraciones Finales.

Este trabajo me ha supuesto un primer contacto real no supervisado con el mundo del análisis de datos. Para llevarlo a cabo me he guiado por los archivos de la asignatura Análisis Estadístico Multivariante proporcionados por los profesores Concepción Domínguez y Jorge Navarro.

La redacción de este informe me ha permitido adquirir experiencia en la interpretación de los resultados, y también en la toma de decisiones informadas conforme a esos resultados que se van obteniendo durante el propio análisis de los datos.
